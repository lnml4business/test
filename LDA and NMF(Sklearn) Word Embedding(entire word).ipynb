{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(r'/Users/allan/Desktop/Tripadvisor Restaurant Data Jan 2017.xlsx', error_bad_lines=False);\n",
    "data_text = data[['Review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(data_text['Review'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<24316x16100 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 809928 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix\n",
    "\n",
    "'''Each of 20k documents is represented as 14546 dimensional \n",
    "vector, which means that our vocabulary has 14546 words.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='batch', learning_offset=10.0,\n",
       "                          max_doc_update_iter=100, max_iter=10,\n",
       "                          mean_change_tol=0.001, n_components=5, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=42, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for topic #0:\n",
      "['atmosphere', 'experience', 'good', 'pizza', 'friendly', 'italian', 'dinner', 'recommend', 'staff', 'amazing', 'menu', 'new', 'delicious', 'excellent', 'best', 'place', 'restaurant', 'service', 'great', 'food']\n",
      "\n",
      "\n",
      "Top 20 words for topic #1:\n",
      "['fries', 'best', 'night', 'beer', 'little', 'pizza', 'wait', 'drinks', 'nice', 'time', 'burger', 'just', 'really', 'like', 'service', 'bar', 'great', 'food', 'place', 'good']\n",
      "\n",
      "\n",
      "Top 20 words for topic #2:\n",
      "['place', 'tasty', 'soup', 'friendly', 'enjoyed', 'fresh', 'wine', 'dinner', 'menu', 'salad', 'chicken', 'lunch', 'excellent', 'nice', 'restaurant', 'delicious', 'great', 'service', 'food', 'good']\n",
      "\n",
      "\n",
      "Top 20 words for topic #3:\n",
      "['like', 'friendly', 'best', 'soon', 'review', 'hope', 'really', 'enjoyed', 'staff', 'visit', 'place', 'restaurant', 'experience', 'thank', 'time', 'breakfast', 'great', 'service', 'good', 'food']\n",
      "\n",
      "\n",
      "Top 20 words for topic #4:\n",
      "['took', 'didn', 'minutes', 'service', 'like', 'dinner', 'waiter', 'good', 'meal', 'got', 'order', 'steak', 'did', 'came', 'just', 'time', 'restaurant', 'food', 'ordered', 'table']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 20 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = tfidf_vect.fit_transform(data_text['Review'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.0, beta_loss='frobenius', init=None, l1_ratio=0.0, max_iter=200,\n",
       "    n_components=5, random_state=42, shuffle=False, solver='cd', tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(doc_term_matrix )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 words for topic #0:\n",
      "['enjoyed', 'got', 'steak', 'order', 'came', 'chicken', 'wait', 'best', 'did', 'delicious', 'menu', 'dinner', 'experience', 'like', 'meal', 'just', 'ordered', 'table', 'time', 'restaurant']\n",
      "\n",
      "\n",
      "Top 20 words for topic #1:\n",
      "['visit', 'fun', 'loved', 'awesome', 'love', 'fantastic', 'highly', 'delicious', 'bar', 'recommend', 'drinks', 'amazing', 'excellent', 'friendly', 'staff', 'atmosphere', 'place', 'service', 'food', 'great']\n",
      "\n",
      "\n",
      "Top 20 words for topic #2:\n",
      "['reasonable', 'tasty', 'restaurant', 'little', 'atmosphere', 'bit', 'quality', 'selection', 'pretty', 'friendly', 'price', 'excellent', 'prices', 'lunch', 'place', 'really', 'nice', 'service', 'food', 'good']\n",
      "\n",
      "\n",
      "Top 20 words for topic #3:\n",
      "['slices', 'like', 've', 'italy', 'salad', 'just', 'friendly', 'sauce', 'delicious', 'toppings', 'fresh', 'cheese', 'pizzas', 'pasta', 'slice', 'best', 'italian', 'crust', 'place', 'pizza']\n",
      "\n",
      "\n",
      "Top 20 words for topic #4:\n",
      "['benedict', 'wait', 'fresh', 'french', 'ate', 'quick', 'bacon', 'buffet', 'hotel', 'diner', 'staff', 'morning', 'toast', 'lunch', 'day', 'friendly', 'pancakes', 'eggs', 'coffee', 'breakfast']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(nmf.components_):\n",
    "    print(f'Top 20 words for topic #{i}:')\n",
    "    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-20:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "data preprocessing(no lematize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(r'/Users/allan/Desktop/Tripadvisor Restaurant Data Jan 2017.xlsx', error_bad_lines=False);\n",
    "data_text = data[['Review']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/allan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tokenization: Split the text into sentences and the sentences into words. \\nLowercase the words and remove punctuation.\\nWords that have fewer than 3 characters are removed.\\nAll stopwords are removed.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v')) \n",
    "#lemmatize: change past to present,stemmer: change to root form\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "'''Tokenization: Split the text into sentences and the sentences into words. \n",
    "Lowercase the words and remove punctuation.\n",
    "Words that have fewer than 3 characters are removed.\n",
    "All stopwords are removed.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_docs = documents['Review'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [second, visit, morton, steakhouse, vegas, loc...\n",
       "1    [restaurant, alley, reminded, little, restaura...\n",
       "2    [went, sunday, brunch, great, time, arrive, ti...\n",
       "3    [freemans, brunch, pretty, exciting, menu, sma...\n",
       "4    [dinner, meeting, colleagues, ambience, perfec...\n",
       "5    [great, expectations, went, christmas, nice, d...\n",
       "6    [gorgeous, place, loved, coming, trip, york, p...\n",
       "7    [hidden, soho, clearly, popular, turs, night, ...\n",
       "8    [food, excellent, course, expensive, restauran...\n",
       "9    [visit, steakhouse, york, menu, contained, gre...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Word embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "path = get_tmpfile(\"word2vec.model\")\n",
    "model = Word2Vec(processed_docs, size=150, window=10, min_count=2, workers=10, iter=10)\n",
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding \n",
    "Generate dictionary only use one word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "s_list=[]\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'service')\n",
    "    if score > 0.38:\n",
    "        s_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "a_list=[]\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'atmosphere')\n",
    "    if score > 0.4:\n",
    "        a_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "v_list=[]\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'value')\n",
    "    if score > 0.4:\n",
    "        v_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "f_list=[]\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'shrimp')\n",
    "    if score > 0.45:\n",
    "        f_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Implicit Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recommend\n",
      "recommended\n",
      "suggest\n",
      "rated\n",
      "screams\n",
      "coordinated\n",
      "recomend\n",
      "recommends\n",
      "repertoire\n",
      "recommendable\n",
      "recomended\n",
      "reccomend\n",
      "mildly\n",
      "nutshell\n",
      "touted\n",
      "interruptions\n",
      "acclaimed\n",
      "commendable\n",
      "absurd\n",
      "pounding\n",
      "wages\n",
      "columbian\n",
      "genres\n",
      "bradbspurs\n",
      "mars\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'recommend')\n",
    "    if score > 0.45:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competent\n",
      "recommend\n",
      "knowledge\n",
      "professional\n",
      "recommended\n",
      "rate\n",
      "pretentious\n",
      "approachable\n",
      "courteous\n",
      "buvette\n",
      "exceptional\n",
      "compromise\n",
      "preparation\n",
      "deliver\n",
      "experienced\n",
      "hardworking\n",
      "extremely\n",
      "encourage\n",
      "rating\n",
      "rated\n",
      "screams\n",
      "unfriendly\n",
      "exceptionally\n",
      "unbelievable\n",
      "polite\n",
      "highest\n",
      "hovering\n",
      "encountered\n",
      "faultless\n",
      "accurate\n",
      "suggesting\n",
      "memorable\n",
      "elegance\n",
      "butts\n",
      "stars\n",
      "genuine\n",
      "repeat\n",
      "coordinated\n",
      "recomend\n",
      "mark\n",
      "fault\n",
      "courtesy\n",
      "terms\n",
      "sensational\n",
      "punctual\n",
      "necessarily\n",
      "patch\n",
      "exemplary\n",
      "grind\n",
      "reflective\n",
      "impressions\n",
      "accomodating\n",
      "extraordinary\n",
      "gratifying\n",
      "recognizes\n",
      "minimal\n",
      "lacking\n",
      "exorbitant\n",
      "friendliness\n",
      "bothersome\n",
      "eager\n",
      "trained\n",
      "outgoing\n",
      "responsive\n",
      "recommending\n",
      "hospitable\n",
      "sufficiently\n",
      "incredibly\n",
      "bellies\n",
      "aways\n",
      "befitting\n",
      "caring\n",
      "megan\n",
      "disinterested\n",
      "personality\n",
      "standpoint\n",
      "richard\n",
      "chasing\n",
      "brusque\n",
      "overbearing\n",
      "ratio\n",
      "unprofessional\n",
      "respectful\n",
      "exudes\n",
      "surprises\n",
      "humor\n",
      "excellence\n",
      "unobtrusive\n",
      "cleanliness\n",
      "flawless\n",
      "deserves\n",
      "enthusiastic\n",
      "receives\n",
      "creates\n",
      "panda\n",
      "measure\n",
      "attentiveness\n",
      "rarely\n",
      "recommendable\n",
      "engage\n",
      "snooty\n",
      "execution\n",
      "friendlier\n",
      "timely\n",
      "unpleasant\n",
      "detracted\n",
      "robbie\n",
      "indicative\n",
      "bravo\n",
      "jamie\n",
      "aspects\n",
      "personable\n",
      "motion\n",
      "tammy\n",
      "recomended\n",
      "exceed\n",
      "conversational\n",
      "intrusive\n",
      "priorities\n",
      "earned\n",
      "kudos\n",
      "pleasurable\n",
      "equals\n",
      "rates\n",
      "marred\n",
      "efficiant\n",
      "skilled\n",
      "impersonal\n",
      "politeness\n",
      "unaccommodating\n",
      "anticipated\n",
      "arrogant\n",
      "reccomend\n",
      "amiable\n",
      "users\n",
      "considerate\n",
      "individuals\n",
      "swift\n",
      "undivided\n",
      "merit\n",
      "understanding\n",
      "mildly\n",
      "excellently\n",
      "professionalism\n",
      "capable\n",
      "interacting\n",
      "pedestrian\n",
      "assisting\n",
      "palomilla\n",
      "referred\n",
      "ranked\n",
      "cheery\n",
      "complimented\n",
      "reflects\n",
      "philippine\n",
      "creativity\n",
      "estimation\n",
      "personnel\n",
      "invisible\n",
      "critique\n",
      "scurried\n",
      "beth\n",
      "anomaly\n",
      "jobs\n",
      "diligent\n",
      "enhanced\n",
      "snuck\n",
      "nutshell\n",
      "absurdly\n",
      "lambchops\n",
      "touted\n",
      "funniest\n",
      "mercy\n",
      "unbeatable\n",
      "analysis\n",
      "ashley\n",
      "teresa\n",
      "cognizant\n",
      "innovation\n",
      "acclaimed\n",
      "enhance\n",
      "basque\n",
      "commendable\n",
      "chummy\n",
      "chloe\n",
      "crabster\n",
      "nationality\n",
      "bugeya\n",
      "gastronomically\n",
      "beaucoup\n",
      "servicing\n",
      "choreographed\n",
      "fanfare\n",
      "insight\n",
      "fussed\n",
      "aplomb\n",
      "gianni\n",
      "courtney\n",
      "employ\n",
      "aided\n",
      "conduct\n",
      "depended\n",
      "nassim\n",
      "neil\n",
      "respects\n",
      "misstep\n",
      "brenda\n",
      "fresher\n",
      "jaime\n",
      "conscientious\n",
      "develop\n",
      "erratic\n",
      "rotis\n",
      "consummate\n",
      "jeffery\n",
      "costumer\n",
      "wages\n",
      "columbian\n",
      "prevents\n",
      "leaders\n",
      "tyler\n",
      "genres\n",
      "madam\n",
      "employment\n",
      "initiative\n",
      "tattooed\n",
      "mondongo\n",
      "contacting\n",
      "bussers\n",
      "spark\n",
      "cody\n",
      "adjustment\n",
      "cherished\n",
      "overtaxed\n",
      "concentrating\n",
      "despina\n",
      "leadership\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'commendable')\n",
    "    if score > 0.45:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word = pd.concat([pd.DataFrame({'food': f_list}), pd.DataFrame({'service':s_list}), pd.DataFrame({'value':v_list}), pd.DataFrame({'atmosphere':a_list})], axis=1)\n",
    "df_word.to_excel(r'/Users/allan/Desktop/dictionary_v1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skills to counter value in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 1, 1, 2]\n",
    "b = [1, 2, 3, 4, 5, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list=[]\n",
    "for x in a:\n",
    "    test_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in b:\n",
    "    test_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 4, 2: 3, 3: 2, 4: 1, 5: 1, 9: 1, 10: 1})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 4, 2: 3, 3: 2, 4: 1, 5: 1, 9: 1, 10: 1}\n"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "for key in test_list:\n",
    "    dict[key] = dict.get(key, 0) + 1\n",
    "print(dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c59229ad1aaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mservice_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mservice_word_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'service'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.38\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "service_list = []\n",
    "service_word_list = []\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'service')\n",
    "    if score > 0.38:\n",
    "        service_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1c7551dd2f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mservice_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mservice_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mservice_score\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.38\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mservice_word_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    for x in service_list:\n",
    "            service_score = model.similarity(word,x)\n",
    "            if service_score > 0.38:\n",
    "                service_word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service       24\n",
       "slammed       21\n",
       "personable    21\n",
       "incredibly    21\n",
       "chummy        21\n",
       "              ..\n",
       "proactive     12\n",
       "obliging      12\n",
       "exudes        12\n",
       "present       12\n",
       "aided         12\n",
       "Length: 300, dtype: int64"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rank and see the result\"\"\"\n",
    "service_result = pd.value_counts(service_word_list)\n",
    "s = service_result.head(300)\n",
    "s = pd.DataFrame(s)\n",
    "s = s._stat_axis.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Food(append food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "food_list = []\n",
    "food_word_list = []\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'shrimp')\n",
    "    if score > 0.45:\n",
    "        food_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    for x in food_list:\n",
    "            food_score = model.similarity(word,x)\n",
    "            if food_score > 0.45:\n",
    "                food_word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shrimp          814\n",
       "shrimps         806\n",
       "blackened       801\n",
       "prawn           800\n",
       "tiger           793\n",
       "               ... \n",
       "scolded         328\n",
       "commonly        328\n",
       "sierra          328\n",
       "marshmallows    328\n",
       "plaintain       328\n",
       "Length: 3000, dtype: int64"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rank and see the result\"\"\"\n",
    "food_result = pd.value_counts(food_word_list)\n",
    "f = food_result.head(3000)\n",
    "f = pd.DataFrame(f)\n",
    "f = f._stat_axis.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "value_list = []\n",
    "value_word_list = []\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'value')\n",
    "    if score > 0.40:\n",
    "        value_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    for x in value_list:\n",
    "            value_score = model.similarity(word,x)\n",
    "            if value_score > 0.40:\n",
    "                value_word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value            109\n",
       "eighteen         100\n",
       "premium           99\n",
       "quantity          98\n",
       "comparison        91\n",
       "                ... \n",
       "discounted        60\n",
       "cheapest          60\n",
       "offbeat           60\n",
       "extraordinary     60\n",
       "metric            60\n",
       "Length: 200, dtype: int64"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rank and see the result\"\"\"\n",
    "value_result = pd.value_counts(value_word_list)\n",
    "v = value_result.head(200)\n",
    "v = pd.DataFrame(v)\n",
    "v = v._stat_axis.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding\n",
    "Atmosphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "atmosphere_list = []\n",
    "atmosphere_word_list = []\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'atmosphere')\n",
    "    if score > 0.45:\n",
    "        atmosphere_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    for x in atmosphere_list:\n",
    "            atmosphere_score = model.similarity(word,x)\n",
    "            if atmosphere_score > 0.40:\n",
    "                atmosphere_word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152727"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(atmosphere_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environment     70\n",
       "atmosphere      70\n",
       "vibe            70\n",
       "stuffy          69\n",
       "surroundings    69\n",
       "                ..\n",
       "singers         54\n",
       "candlelight     54\n",
       "themed          54\n",
       "hubbub          54\n",
       "dancer          54\n",
       "Length: 300, dtype: int64"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rank and see the result\"\"\"\n",
    "atmosphere_result = pd.value_counts(atmosphere_word_list)\n",
    "a = atmosphere_result.head(300)\n",
    "a = pd.DataFrame(a)\n",
    "a = a._stat_axis.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repurchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visit\n",
      "recommend\n",
      "come\n",
      "enjoyable\n",
      "return\n",
      "captured\n",
      "returning\n",
      "stopping\n",
      "keeper\n",
      "revisit\n",
      "woukd\n",
      "beth\n",
      "commendable\n",
      "carol\n"
     ]
    }
   ],
   "source": [
    "repurchase_list = []\n",
    "repurchase_word_list = []\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'return')\n",
    "    if score > 0.45:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(repurchase_list, columns = ['most_similar_Word'])\n",
    "a.to_csv('/Users/allan/Desktop/repurchase/1_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    for x in repurchase_list:\n",
    "            repurchase_score = model.similarity(word,x)\n",
    "            if repurchase_score > 0.40:\n",
    "                repurchase_word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "returning        12\n",
       "return           12\n",
       "repeat           11\n",
       "myers            10\n",
       "adriano           9\n",
       "stopping          9\n",
       "carol             9\n",
       "users             8\n",
       "deserving         8\n",
       "recommending      8\n",
       "jennifer          8\n",
       "regret            8\n",
       "mamas             8\n",
       "appreciative      8\n",
       "priority          8\n",
       "ironing           8\n",
       "choosing          8\n",
       "weeklong          8\n",
       "fran              7\n",
       "circumstance      7\n",
       "arise             7\n",
       "markteam          7\n",
       "revisit           7\n",
       "trips             7\n",
       "cowfish           7\n",
       "dine              7\n",
       "conscious         7\n",
       "honored           7\n",
       "trendiest         7\n",
       "carla             7\n",
       "gouge             7\n",
       "sean              7\n",
       "coming            7\n",
       "rethink           7\n",
       "travis            7\n",
       "rico              7\n",
       "noho              7\n",
       "giorgio           7\n",
       "nora              7\n",
       "overjoyed         7\n",
       "complimenting     7\n",
       "satisfactory      7\n",
       "pass              7\n",
       "joining           7\n",
       "geremew           7\n",
       "future            7\n",
       "waistline         7\n",
       "lolinda           7\n",
       "susann            7\n",
       "shall             7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rank and see the result\"\"\"\n",
    "repurchase_result = pd.value_counts(repurchase_word_list)\n",
    "repurchase_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visit',\n",
       " 'coming',\n",
       " 'come',\n",
       " 'return',\n",
       " 'plan',\n",
       " 'repeat',\n",
       " 'returning',\n",
       " 'stopping',\n",
       " 'keeper',\n",
       " 'revisit',\n",
       " 'beth',\n",
       " 'bradbspurs']"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repurchase_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "recommend_list = []\n",
    "recommend_word_list = []\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'recommend')\n",
    "    if score > 0.45:\n",
    "        recommend_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i, word in enumerate(words):\n",
    "    #score = model.similarity(word,'tasty')\n",
    "    #if score > 0.50:\n",
    "        #recommend_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    for x in recommend_list:\n",
    "            recommend_score = model.similarity(word,x)\n",
    "            if recommend_score > 0.40:\n",
    "                recommend_word_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recomended       25\n",
       "recommend        25\n",
       "recommendable    24\n",
       "touted           24\n",
       "screams          23\n",
       "                 ..\n",
       "wayfarer         11\n",
       "worlds           11\n",
       "fudge            11\n",
       "concentrating    11\n",
       "privilege        11\n",
       "Length: 300, dtype: int64"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"rank and see the result\"\"\"\n",
    "recommend_result = pd.value_counts(recommend_word_list)\n",
    "r = recommend_result.head(300)\n",
    "r = pd.DataFrame(r)\n",
    "r = r._stat_axis.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('negation.pickle', 'wb') as b:\n",
    "    pickle.dump(f,b)\n",
    "    pickle.dump(s,b)\n",
    "    pickle.dump(v,b)\n",
    "    pickle.dump(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_dictionary = pd.DataFrame()\n",
    "df_dictionary['Food'] = f\n",
    "df_dictionary['Service'] = s\n",
    "df_dictionary['Value'] = v\n",
    "df_dictionary['Atmosphere'] = a\n",
    "df_dictionary['Implicit Recommendation'] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dictionary.to_excel(r'/Users/allan/Desktop/dictionary_v2.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f_list.append('food')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dictionary v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_f_list = f._stat_axis.values.tolist()\n",
    "new_s_list = s._stat_axis.values.tolist()\n",
    "new_v_list = v._stat_axis.values.tolist()\n",
    "new_a_list = a._stat_axis.values.tolist()\n",
    "new_r_list = r._stat_axis.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>food</th>\n",
       "      <th>service</th>\n",
       "      <th>value</th>\n",
       "      <th>atmosphere</th>\n",
       "      <th>Implicit Recommendation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>shrimp</td>\n",
       "      <td>service</td>\n",
       "      <td>value</td>\n",
       "      <td>environment</td>\n",
       "      <td>recomended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shrimps</td>\n",
       "      <td>slammed</td>\n",
       "      <td>eighteen</td>\n",
       "      <td>atmosphere</td>\n",
       "      <td>recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackened</td>\n",
       "      <td>personable</td>\n",
       "      <td>premium</td>\n",
       "      <td>vibe</td>\n",
       "      <td>recommendable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prawn</td>\n",
       "      <td>incredibly</td>\n",
       "      <td>quantity</td>\n",
       "      <td>stuffy</td>\n",
       "      <td>touted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tiger</td>\n",
       "      <td>chummy</td>\n",
       "      <td>comparison</td>\n",
       "      <td>surroundings</td>\n",
       "      <td>screams</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>scolded</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>commonly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>sierra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>marshmallows</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>plaintain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              food     service       value    atmosphere  \\\n",
       "0           shrimp     service       value   environment   \n",
       "1          shrimps     slammed    eighteen    atmosphere   \n",
       "2        blackened  personable     premium          vibe   \n",
       "3            prawn  incredibly    quantity        stuffy   \n",
       "4            tiger      chummy  comparison  surroundings   \n",
       "...            ...         ...         ...           ...   \n",
       "2995       scolded         NaN         NaN           NaN   \n",
       "2996      commonly         NaN         NaN           NaN   \n",
       "2997        sierra         NaN         NaN           NaN   \n",
       "2998  marshmallows         NaN         NaN           NaN   \n",
       "2999     plaintain         NaN         NaN           NaN   \n",
       "\n",
       "     Implicit Recommendation  \n",
       "0                 recomended  \n",
       "1                  recommend  \n",
       "2              recommendable  \n",
       "3                     touted  \n",
       "4                    screams  \n",
       "...                      ...  \n",
       "2995                     NaN  \n",
       "2996                     NaN  \n",
       "2997                     NaN  \n",
       "2998                     NaN  \n",
       "2999                     NaN  \n",
       "\n",
       "[3000 rows x 5 columns]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dic_v3 = pd.concat([pd.DataFrame({'food': new_f_list}), pd.DataFrame({'service':new_s_list}), pd.DataFrame({'value':new_v_list}), pd.DataFrame({'atmosphere':new_a_list}), pd.DataFrame({'Implicit Recommendation':new_r_list})], axis=1)\n",
    "df_dic_v3.to_excel(r'/Users/allan/Desktop/work/dictionary_v3.xlsx',index=False)\n",
    "df_dic_v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great\n",
      "perfect\n",
      "terrific\n",
      "good\n",
      "super\n",
      "loved\n",
      "delicious\n",
      "excellent\n",
      "enjoyed\n",
      "nice\n",
      "lovely\n",
      "lots\n",
      "cool\n",
      "love\n",
      "wonderful\n",
      "amazing\n",
      "fantastic\n",
      "relaxed\n",
      "decent\n",
      "superb\n",
      "plenty\n",
      "lively\n",
      "outstanding\n",
      "enjoy\n",
      "awesome\n",
      "fabulous\n",
      "solid\n",
      "incredible\n",
      "phenomenal\n",
      "brilliant\n",
      "notch\n",
      "beat\n",
      "diminished\n",
      "helpfull\n",
      "foley\n"
     ]
    }
   ],
   "source": [
    "posdict = []\n",
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'great')\n",
    "    if score > 0.45:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/allan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disappointed\n",
      "reading\n",
      "based\n",
      "excited\n",
      "impressed\n",
      "disappointing\n",
      "rave\n",
      "underwhelmed\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(words):\n",
    "    score = model.similarity(word,'disappointed')\n",
    "    if score > 0.55:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#è½½å…¥æƒ…æ„Ÿè¯å…¸\n",
    "posdict=[]\n",
    "negdict=[]\n",
    "verydict=[]\n",
    "insufficientdict=[]\n",
    "inverdict=[]\n",
    "pos=open('/Users/RyanYang/PycharmProjects/linguistic_analysis/sentiment_dictionary/positive.txt')\n",
    "for line in pos:\n",
    "    posdict.append(line.rstrip('\\n'))\n",
    "neg=open('/Users/RyanYang/PycharmProjects/linguistic_analysis/sentiment_dictionary/negative.txt', 'rU')\n",
    "for line in neg:\n",
    "    negdict.append(line.rstrip('\\n'))\n",
    "very=open('/Users/RyanYang/PycharmProjects/linguistic_analysis/sentiment_dictionary/very.txt', 'rU')\n",
    "for line in very:\n",
    "    verydict.append(line.rstrip('\\n'))\n",
    "insufficient=open('/Users/RyanYang/PycharmProjects/linguistic_analysis/sentiment_dictionary/insufficient.txt', 'rU')\n",
    "for line in insufficient:\n",
    "    insufficientdict.append(line.rstrip('\\n'))\n",
    "inver=open('/Users/RyanYang/PycharmProjects/linguistic_analysis/sentiment_dictionary/inverse.txt', 'rU')\n",
    "for line in inver:\n",
    "    inverdict.append(line.rstrip('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import csv\n",
    "import time\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "\n",
    "\n",
    "#è½½å…¥è¯„è®º\n",
    "review = open('/Users/RyanYang/PycharmProjects/linguistic_analysis/Valence/extract_data/value_2016.txt', 'rU')\n",
    "\n",
    "#åˆ¤å®šå¥‡æ•°å¶æ•°æ—¶ä½¿ç”¨:\n",
    "def judgeodd(num):\n",
    "    if num & 1 == 0:\n",
    "        return 'even'\n",
    "    else:\n",
    "        return 'odd'\n",
    "\n",
    "#ä¸»ç¨‹åº\n",
    "def sentiment_score_list(dataset):\n",
    "    cuted_data=[]\n",
    "    for cell in dataset:\n",
    "        cuted_data.append(cell.split())\n",
    "\n",
    "    count1=[]\n",
    "    count2=[]\n",
    "    count3=[]\n",
    "\n",
    "    for sent in cuted_data:\n",
    "        len_sent = len(sent)\n",
    "        i=0  # æ‰«æè¯çš„ä½ç½®\n",
    "        a=0\n",
    "        poscount = 0  # ç§¯æžè¯çš„ç¬¬ä¸€æ¬¡åˆ†å€¼\n",
    "        poscount2 = 0  # ç§¯æžè¯åè½¬åŽçš„åˆ†å€¼\n",
    "        poscount3 = 0  # ç§¯æžè¯çš„æœ€åŽåˆ†å€¼ï¼ˆåŒ…æ‹¬å¹å·çš„åˆ†å€¼ï¼‰\n",
    "        negcount = 0\n",
    "        negcount2 = 0\n",
    "        negcount3 = 0\n",
    "        for word in sent:\n",
    "            if word in posdict:\n",
    "                poscount += 1\n",
    "                c = 0\n",
    "                for w in sent[a:i]:\n",
    "                    if w in verydict:\n",
    "                        poscount *= 2.0\n",
    "                    elif w in insufficientdict:\n",
    "                        poscount /= 2.0\n",
    "                    elif w in inverdict:\n",
    "                        c += 1\n",
    "                if judgeodd(c) == 'odd':  # å¦‚æžœæƒ…æ„Ÿè¯å‰inverseæ¬¡æ•°ä¸ºå¥‡æ•°\n",
    "                    poscount *= -1.0\n",
    "                    poscount2 += poscount\n",
    "                    poscount = 0\n",
    "                    poscount3 += poscount2\n",
    "                    poscount2 = 0\n",
    "\n",
    "                else:\n",
    "                    poscount3 += poscount\n",
    "                    poscount = 0\n",
    "\n",
    "                a = i + 1  # æƒ…æ„Ÿè¯ä½ç½®å‰ç§»\n",
    "\n",
    "            elif word in negdict:\n",
    "                negcount += 1\n",
    "                d=0\n",
    "                for w in sent[a:i]:\n",
    "                    if w in verydict:\n",
    "                        negcount *= 2.0\n",
    "                    elif w in insufficientdict:\n",
    "                        negcount /= 2.0\n",
    "                    elif w in inverdict:\n",
    "                        d += 1\n",
    "\n",
    "                if judgeodd(d) == 'odd':  # å¦‚æžœæƒ…æ„Ÿè¯å‰inverseæ¬¡æ•°ä¸ºå¥‡æ•°\n",
    "                    negcount *= -1.0\n",
    "                    negcount2 += negcount\n",
    "                    negcount = 0\n",
    "                    negcount3 += negcount2\n",
    "                    negcount2 = 0\n",
    "\n",
    "                else:\n",
    "                    negcount3 += negcount\n",
    "                    negcount = 0\n",
    "\n",
    "                a = i + 1\n",
    "\n",
    "            i += 1  # æ‰«æä½ç½®å‰ç§»\n",
    "\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        if poscount3 < 0 and negcount3 >= 0:\n",
    "            neg_count = negcount3 - poscount3\n",
    "            pos_count = 0\n",
    "        elif negcount3 < 0 and poscount3 > 0:\n",
    "            pos_count = poscount3 - negcount3\n",
    "            neg_count = 0\n",
    "        elif poscount3 < 0 and negcount3 < 0:\n",
    "            neg_count = -poscount3\n",
    "            pos_count = -negcount3\n",
    "        else:\n",
    "            pos_count = poscount3\n",
    "            neg_count = negcount3\n",
    "\n",
    "        score = pos_count-neg_count\n",
    "        final_score = score/(len_sent)\n",
    "        count1.append(final_score)\n",
    "        count3.append(len_sent)\n",
    "\n",
    "    count2.append(count1)\n",
    "    #count1 = []\n",
    "\n",
    "    return count1, count3\n",
    "\n",
    "#def sentiment_score(senti_score_list):\n",
    "#    score = []\n",
    "#    for rev in senti_score_list:\n",
    "#        score_array = np.array(rev)\n",
    "#        Pos = np.sum(score_array[:, 0])\n",
    "#        Neg = np.sum(score_array[:, 1])\n",
    "#        AvgPos = np.mean(score_array[:, 0])\n",
    "#        AvgNeg = np.mean(score_array[:, 1])\n",
    "#        StdPos = np.std(score_array[:, 0])\n",
    "#        StdNeg = np.std(score_array[:, 1])\n",
    "#        score.append([Pos, Neg, AvgPos, AvgNeg, StdPos, StdNeg])\n",
    "#    return score\n",
    "\n",
    "sentiment_list = sentiment_score_list(review)\n",
    "sentiment = sentiment_list[0]\n",
    "length = sentiment_list[1]\n",
    "\n",
    "\n",
    "with open ('value_valence_2016.csv', 'w') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    for row in zip(sentiment, length):\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "#final_score = sentiment_score (sentiment_list)\n",
    "print(time.clock() - start_time, \"seconds\")\n",
    "#print final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count skills "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "a = Counter(service_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "service          24\n",
       "slammed          21\n",
       "personable       21\n",
       "incredibly       21\n",
       "chummy           21\n",
       "extremely        21\n",
       "polite           21\n",
       "russia           20\n",
       "waitstaff        20\n",
       "smiling          20\n",
       "manner           20\n",
       "timely           20\n",
       "sever            19\n",
       "buzzy            19\n",
       "hebert           19\n",
       "waitresses       19\n",
       "servers          19\n",
       "informative      19\n",
       "patient          19\n",
       "staffs           19\n",
       "timing           19\n",
       "cheerful         19\n",
       "organized        19\n",
       "waiters          19\n",
       "unobtrusive      18\n",
       "sevice           18\n",
       "humored          18\n",
       "engaging         18\n",
       "busboys          18\n",
       "professional     18\n",
       "jalisco          18\n",
       "greeting         18\n",
       "jobs             18\n",
       "intrusive        18\n",
       "hovering         18\n",
       "competent        18\n",
       "goran            18\n",
       "schedule         18\n",
       "paced            18\n",
       "overbearing      18\n",
       "funny            18\n",
       "teamwork         18\n",
       "chatty           18\n",
       "knowledgable     18\n",
       "advising         18\n",
       "shannon          18\n",
       "courteous        18\n",
       "hostesses        17\n",
       "answering        17\n",
       "accommodating    17\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_result = pd.value_counts(service_word_list)\n",
    "service_result.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
